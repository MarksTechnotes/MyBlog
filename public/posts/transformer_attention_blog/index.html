<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Demystifying Transformers: Attention, Positional Encoding, and Feed-Forward Networks | Mark’s Tech Notes</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="A clear and intuitive guide to Transformer architecture, covering attention mechanisms, Q/K/V projections, multi-head attention, feed-forward networks, residuals, layer normalization, and sinusoidal positional encoding.">
    <meta name="generator" content="Hugo 0.152.2">
    
    
    
      <meta name="robots" content="index, follow">
    
    

    
<link rel="stylesheet" href="/MyBlog/ananke/css/main.min.efe4d852f731d5d1fbb87718387202a97aafd768cdcdaed0662bbe6982e91824.css" >




    


    
      

    

    

    
      <link rel="canonical" href="https://markstechnotes.github.io/MyBlog/posts/transformer_attention_blog/">
    

    <meta property="og:url" content="https://markstechnotes.github.io/MyBlog/posts/transformer_attention_blog/">
  <meta property="og:site_name" content="Mark’s Tech Notes">
  <meta property="og:title" content="Demystifying Transformers: Attention, Positional Encoding, and Feed-Forward Networks">
  <meta property="og:description" content="A clear and intuitive guide to Transformer architecture, covering attention mechanisms, Q/K/V projections, multi-head attention, feed-forward networks, residuals, layer normalization, and sinusoidal positional encoding.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-19T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-19T00:00:00+00:00">
    <meta property="article:tag" content="Transformers">
    <meta property="article:tag" content="Attention">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Deep Learning">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="GPT">

  <meta itemprop="name" content="Demystifying Transformers: Attention, Positional Encoding, and Feed-Forward Networks">
  <meta itemprop="description" content="A clear and intuitive guide to Transformer architecture, covering attention mechanisms, Q/K/V projections, multi-head attention, feed-forward networks, residuals, layer normalization, and sinusoidal positional encoding.">
  <meta itemprop="datePublished" content="2025-11-19T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-11-19T00:00:00+00:00">
  <meta itemprop="wordCount" content="833">
  <meta itemprop="keywords" content="Transformers,Attention,Machine Learning,Deep Learning,NLP,GPT,BERT">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Demystifying Transformers: Attention, Positional Encoding, and Feed-Forward Networks">
  <meta name="twitter:description" content="A clear and intuitive guide to Transformer architecture, covering attention mechanisms, Q/K/V projections, multi-head attention, feed-forward networks, residuals, layer normalization, and sinusoidal positional encoding.">

      
    
	
  </head><body class="ma0 avenir bg-near-white production">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/MyBlog/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Mark’s Tech Notes
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">Demystifying Transformers: Attention, Positional Encoding, and Feed-Forward Networks</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-11-19T00:00:00Z">November 19, 2025</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Transformers have become the foundation of modern language models like GPT, PaLM, and LLaMA. But their internal workings — attention heads, Q/K/V projections, sinusoidal positional encodings, feed-forward networks — can feel overwhelming. Ironically or probably aptly, I used ChatGPT which is based on the Transformers architecture to help explain to me the concepts and it did a great job.<br>
This post which is a summary of the sessions I had with ChatGPT explains these concepts clearly and intuitively.</p>
<hr>
<h2 id="1-tokens-and-embeddings">1. Tokens and Embeddings</h2>
<p>Transformers do not process raw words. Each token (word/piece of a word) is converted into a <strong>vector</strong>, called an embedding.<br>
Example:</p>
<pre tabindex="0"><code>“The”  →  [0.12, 0.87, …, -0.33]   (512-dimensional vector)
</code></pre><p>Each dimension represents some latent feature learned during training.</p>
<hr>
<h2 id="2-the-attention-mechanism-q-k-and-v">2. The Attention Mechanism: Q, K, and V</h2>
<p>Inside each layer, every token is transformed into <strong>three different vectors</strong>:</p>
<ul>
<li><strong>Q</strong> (Query)</li>
<li><strong>K</strong> (Key)</li>
<li><strong>V</strong> (Value)</li>
</ul>
<p>These are computed using three learned matrices:</p>
<p>$$
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
$$</p>
<p>Where $X$ is the token embedding.</p>
<p>So what’s the difference between Q, K, and V?</p>
<ul>
<li><strong>Q asks:</strong> “What am I looking for?”</li>
<li><strong>K answers:</strong> “What do I contain?”</li>
<li><strong>V carries:</strong> “Here is my information.”</li>
</ul>
<h3 id="how-attention-works">How attention works</h3>
<p>To compute how much one token should attend to another, we take:</p>
<p>$$
\text{score} = Q_i \cdot K_j
$$</p>
<p>Then normalize with softmax.<br>
Finally, each token receives a <strong>context vector</strong>:</p>
<p>$$
\text{Context}_i = \sum_j \text{attention}(i \to j) \cdot V_j
$$</p>
<p>This allows each token to gather information from all other tokens.</p>
<hr>
<h2 id="3-why-multi-head-attention">3. Why Multi-Head Attention?</h2>
<p>Instead of doing attention once, Transformers do it <strong>many times in parallel</strong>, using different learned projections.</p>
<p>Example:</p>
<ul>
<li>Embedding size = 512</li>
<li>8 heads</li>
<li>Each head uses 64-dimensional Q/K/V vectors</li>
</ul>
<p>Each head learns a different type of pattern:</p>
<ul>
<li>subject–verb relations</li>
<li>long-range dependencies</li>
<li>punctuation context</li>
<li>positional relations</li>
</ul>
<p>The heads are concatenated and mixed using another learned matrix $W_O$.</p>
<p>Multi-head attention = multiple “specialists,” each attending differently.</p>
<hr>
<h2 id="4-the-feed-forward-network-ffn">4. The Feed-Forward Network (FFN)</h2>
<p>Attention lets tokens <strong>communicate</strong>, but the model also needs the ability to <strong>transform</strong> each token individually — add non-linearity, mix dimensions, build abstraction.</p>
<p>For this, every Transformer layer includes:</p>
<p>$$
\text{FFN}(x) = \max(0, x W_1 + b_1) W_2 + b_2
$$</p>
<ul>
<li><strong>W₁</strong> expands dimensionality (e.g., 512 → 2048)</li>
<li><strong>ReLU</strong> adds non-linearity</li>
<li><strong>W₂</strong> projects back down (2048 → 512)</li>
</ul>
<p>This is a tiny neural network applied <strong>independently to each token</strong>.</p>
<p>Why expand to 2048?<br>
A larger hidden dimension lets the model mix and recombine features more powerfully before compressing back.</p>
<p>Attention = talk to other tokens<br>
FFN = think for yourself</p>
<hr>
<h2 id="5-residual-connections-and-layer-norm">5. Residual Connections and Layer Norm</h2>
<p>Every sublayer (attention and FFN) is wrapped with:</p>
<ul>
<li>a residual connection</li>
<li>layer normalization</li>
</ul>
<p>Example:</p>
<p>$$
x \to \text{Attention}(x) + x \to \text{LayerNorm}
$$</p>
<p>Residuals help gradients flow and stabilize training.<br>
LayerNorm stabilizes activations.</p>
<hr>
<h2 id="6-positional-encoding-why-we-need-it">6. Positional Encoding: Why We Need It</h2>
<p>Attention is order-agnostic — without additional info, the model cannot tell:</p>
<ul>
<li>“cat chased dog”<br>
from</li>
<li>“dog chased cat”</li>
</ul>
<p>So Transformers add a <strong>positional encoding</strong> to each token embedding.</p>
<hr>
<h2 id="7-sinusoidal-positional-encoding">7. Sinusoidal Positional Encoding</h2>
<p>The original Transformer uses fixed sinusoidal functions:</p>
<p>$$
PE(p, 2i)   = \sin\left(\frac{p}{10000^{2i / d_{model}}}\right), \quad
PE(p, 2i+1) = \cos\left(\frac{p}{10000^{2i / d_{model}}}\right)
$$</p>
<p>Here:</p>
<ul>
<li>$p$ = token position</li>
<li>each dimension gets a different <strong>frequency</strong></li>
<li>lower dimensions = short wavelength (capture fine details)</li>
<li>higher dimensions = long wavelength (capture coarse/global structure)</li>
</ul>
<p>This creates a multi-frequency “positional signature.”</p>
<h3 id="why-use-many-frequencies">Why use many frequencies?</h3>
<p>If we added the same number to every dimension (e.g., “add 1 for position 1”), the model would not know:</p>
<ul>
<li>how far apart tokens are</li>
<li>or capture hierarchical structure</li>
<li>or generalize to longer sequences</li>
</ul>
<p>Sinusoids at different frequencies form a <strong>Fourier-like basis</strong> that lets the model infer both absolute and relative positions.</p>
<p><em>Note: The original paper did not reference Fourier analysis; this interpretation came later.</em></p>
<hr>
<h2 id="8-encoder-vs-decoder">8. Encoder vs. Decoder</h2>
<p>In sequence-to-sequence tasks (e.g., translation):</p>
<h3 id="encoder">Encoder</h3>
<ul>
<li>reads the input sentence</li>
<li>uses <em>self-attention</em></li>
<li>outputs context-rich representations</li>
</ul>
<h3 id="decoder">Decoder</h3>
<ul>
<li>generates output tokens one at a time</li>
<li>uses <strong>masked self-attention</strong> (prevents looking at future tokens during training)</li>
<li>uses <strong>cross-attention</strong> (looks at encoder outputs)</li>
</ul>
<p>GPT-style models remove the encoder entirely and use only the decoder with masked self-attention.</p>
<hr>
<h2 id="9-why-masked-self-attention">9. Why Masked Self-Attention?</h2>
<p>Even during <strong>training</strong>, the model is given the full output sequence.<br>
Without a mask, the decoder would “cheat” by looking at future tokens.</p>
<p>Example: predicting “world” in “Hello world”: the model must not see “world” when trying to predict it.</p>
<p>Masking ensures autoregressive generation is learned correctly.</p>
<hr>
<h2 id="10-summary-how-a-transformer-layer-works">10. Summary: How a Transformer Layer Works</h2>
<p>For each layer:</p>
<ol>
<li><strong>(Decoder only) Masked self-attention</strong></li>
<li><strong>Self-attention</strong> (or cross-attention in the decoder)</li>
<li><strong>Feed-forward network</strong></li>
<li>Residuals + LayerNorm everywhere</li>
</ol>
<p>This stack is repeated many times (12+, 24+, 48+ layers).</p>
<p>Overall pattern:</p>
<ul>
<li>Tokens look at each other → attention</li>
<li>Tokens process their own updated representations → FFN</li>
<li>Repeat across layers → deep understanding</li>
</ul>
<hr>
<h2 id="conclusion">Conclusion</h2>
<p>Transformers work by combining:</p>
<ul>
<li>multi-head attention (token-to-token interaction)</li>
<li>feed-forward networks (token-wise transformation)</li>
<li>positional encoding (ordering)</li>
<li>residuals + normalization (stability)</li>
</ul>
<p>Attention handles <strong>relationships</strong>.<br>
FFNs handle <strong>representations</strong>.<br>
Positional encodings handle <strong>order</strong>.<br>
Stacking layers builds <strong>deep abstraction</strong>.</p>
<ul class="pa0">
  
   <li class="list di">
     <a href="/MyBlog/tags/transformers/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Transformers</a>
   </li>
  
   <li class="list di">
     <a href="/MyBlog/tags/attention/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Attention</a>
   </li>
  
   <li class="list di">
     <a href="/MyBlog/tags/machine-learning/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Machine Learning</a>
   </li>
  
   <li class="list di">
     <a href="/MyBlog/tags/deep-learning/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Deep Learning</a>
   </li>
  
   <li class="list di">
     <a href="/MyBlog/tags/nlp/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">NLP</a>
   </li>
  
   <li class="list di">
     <a href="/MyBlog/tags/gpt/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">GPT</a>
   </li>
  
   <li class="list di">
     <a href="/MyBlog/tags/bert/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">BERT</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="https://markstechnotes.github.io/MyBlog/" >
    &copy;  Mark’s Tech Notes 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
